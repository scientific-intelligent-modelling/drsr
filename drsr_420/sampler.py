# Copyright 2023 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

""" Class for sampling new program skeletons. """
from __future__ import annotations
from abc import ABC, abstractmethod

from typing import Collection, Sequence, Type
import numpy as np
import time

import random
from drsr_420 import evaluator
from drsr_420 import buffer
from drsr_420 import config as config_lib
import requests
import json
# http.client ä¸å†ä½¿ç”¨ï¼Œè°ƒç”¨ç»Ÿä¸€çš„ llm.ClientFactory
import os
import traceback
from typing import Any

# ç®€å•æ¸…æ´—ï¼šä¿ç•™åˆ°é¦–ä¸ª return ä¸ºæ­¢ï¼Œç§»é™¤å±é™©/æ— å…³è¡Œï¼Œå¹¶ç¡®ä¿ç¼©è¿›
def _sanitize_equation_body(text: str) -> str:
    if not isinstance(text, str):
        return ""
    lines = text.splitlines()
    cleaned = []
    found_return = False
    blacklist = (
        'import ', 'print(', 'open(', 'os.', 'sys.', '__import__', 'subprocess', 'eval(', 'exec(', 'if __name__', 'while True:'
    )
    for raw in lines:
        s = raw.strip()
        if not s:
            # è·³è¿‡å¼€å¤´çš„ç©ºè¡Œ
            if not cleaned:
                continue
        # é»‘åå•è¿‡æ»¤
        lower_line = s.lower()
        if any(tok in lower_line for tok in blacklist):
            continue
        cleaned.append(('    ' + s) if s else s)  # ç¡®ä¿åŸºæœ¬ç¼©è¿›
        if s.startswith('return'):
            found_return = True
            break
    # è‹¥æ²¡æœ‰ returnï¼Œå°½é‡è¿”å›æ¸…ç†åçš„å†…å®¹
    return "\n".join(cleaned) + ("\n" if cleaned else "")

SHARED_LLM_CLIENT: Any = None

def set_shared_llm_client(client):
    global SHARED_LLM_CLIENT
    SHARED_LLM_CLIENT = client
problem_name_in_prompt = 'a damped nonlinear oscillator system with driving force'
dependent_name_in_prompt = 'acceleration'
independent_name_in_prompt = 'position, and velocity'
Port = '5000'

# é‡‡æ ·ä¸åˆ†ææ—¶çš„æœ€å¤§è¾“å‡º tokenï¼›æ¨¡å‹åéœ€ç”±å¤–éƒ¨ config æ˜¾å¼æä¾›
class LLM(ABC):
    def __init__(self, samples_per_prompt: int) -> None:
        self._samples_per_prompt = samples_per_prompt

    def _draw_sample(self, prompt: str) -> str:
        """ Return a predicted continuation of `prompt`."""
        raise NotImplementedError('Must provide a language model.')

    @abstractmethod
    def draw_samples(self, prompt: str) -> Collection[str]:
        """ Return multiple predicted continuations of `prompt`. """
        return [self._draw_sample(prompt) for _ in range(self._samples_per_prompt)]
    # self._samples_per_prompt = 4 æ¯ä¸€æ¬¡promptéƒ½ç”Ÿæˆå››ä¸ªç›¸äº’ç‹¬ç«‹çš„å›ç­”



class Sampler:
    """ Node that samples program skeleton continuations and sends them for analysis. """
    _global_samples_nums: int = 1 

    def __init__(
            self,
            database: buffer.ExperienceBuffer,
            evaluators: Sequence[evaluator.Evaluator],
            samples_per_prompt: int,
            config: config_lib.Config,
            max_sample_nums: int | None = None,
            llm_class: Type[LLM] = LLM,
    ):
        self._samples_per_prompt = samples_per_prompt
        self._database = database
        self._evaluators = evaluators
        self._llm = llm_class(samples_per_prompt)
        self._max_sample_nums = max_sample_nums
        self.config = config

# python main.py --problem_name oscillator1 --spec_path ./specs/specification_oscillator1_numpy.txt --log_path ./logs/oscillator1_local
    def sample(self, **kwargs):
        """ Continuously gets prompts, samples programs, sends them for analysis. """
        while True:
            # stop the search process if hit global max sample nums
            if self._max_sample_nums and self.__class__._global_samples_nums >= self._max_sample_nums:
                break
            
            prompt = self._database.get_prompt()    # ä»å²›ä¸Šæ‹¿ä¸€ä¸ªå¯å‚è€ƒçš„æ–¹ç¨‹æ¡†æ¶ - æ•…å¯ä»¥ç‹¬ç«‹åæ€

            island_id = prompt.island_id

            best_score = self._database._best_score_per_island[island_id]
            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ¯ä¸ªå²›å±¿çš„promptè·å–ä¿¡æ¯
            # print(f"ä»å²›å±¿ {island_id} è·å–promptï¼Œæœ€ä½³åˆ†æ•°: {best_score}")

            reset_time = time.time()

            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºLLMè°ƒç”¨ä¿¡æ¯
            # print("è°ƒç”¨å¤§æ¨¡å‹å¤„ç†")

            # 01 ç‰ˆæœ¬
            # samples, sed_rep = self._llm.draw_samples(prompt.code,self.config) # å‘å¤§æ¨¡å‹é‡‡æ ·å‡ºä¸€ä¸ªæ–¹ç¨‹æ¡†æ¶ - æ ¸å¿ƒ
            samples = self._llm.draw_samples(prompt.code,self.config) # å‘å¤§æ¨¡å‹é‡‡æ ·å‡ºä¸€ä¸ªæ–¹ç¨‹æ¡†æ¶ - æ ¸å¿ƒ
  
            sample_time = (time.time() - reset_time) / self._samples_per_prompt

            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºé‡‡æ ·ç»“æœ
            # print("è·å¾—äº†samplesï¼Œåœ¨95è¡Œ")
            # print(samples)
            # This loop can be executed in parallel on remote evaluator machines.
            score_for_sample = []
            error_for_samlple = []
            opt_params_for_sample = []
            quality_for_sample = []
            residual_data = None  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„æ®‹å·®æ•°æ®
            best_sample = None
            if_best = False
            id = 0
            temp_best_score = []
            for sample in samples:
                self._global_sample_nums_plus_one()
                cur_global_sample_nums = self._get_global_sample_nums()
                chosen_evaluator: evaluator.Evaluator = np.random.choice(self._evaluators)
                # å…ˆæ¸…æ´—æ ·æœ¬ï¼Œå»é™¤æµ‹è¯•ä»£ç /å±é™©è¯­å¥ï¼Œä»…ä¿ç•™å‡½æ•°ä½“ç‰‡æ®µ
                sample_clean = _sanitize_equation_body(sample)
                score, error_msg, residual, opt_params = chosen_evaluator.analyse(
                    sample_clean,
                    prompt.island_id,
                    prompt.version_generated,
                    **kwargs,
                    global_sample_nums=cur_global_sample_nums,
                    sample_time=sample_time
                )
                score_for_sample.append(score)
                error_for_samlple.append(error_msg)
                opt_params_for_sample.append(opt_params)
                id += 1
                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ¯ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»†èŠ‚
                # print(best_score)
                # print(score)
                # print('===================ä»chosen_evaluator.analyseä¸­è·å¾—æ®‹å·®=====================\n')
                # print(residual)
                if score is not None and score > best_score:
                # if score is not None :#å…ˆä¸ºäº†è°ƒè¯•ï¼Œéƒ½æä¸€éï¼Œä¸Šé¢çš„æ‰æ˜¯éœ€è¦çš„
                    temp_best_score.append(score)
                    #å¦‚æœscoreæ¯”temp_best_scoreä¸­çš„æœ€å¤§å€¼å¤§ï¼Œå°±æ›´æ–°best
                    if score >= max(temp_best_score):
                        best_id = id
                        if_best = True
                        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        # print("æˆ‘åœ¨è¿™é‡Œå˜æˆtrueäº†")
                        residual_data=residual
                        best_sample = sample
                        best_score_for_sample = score

                        # æ·»åŠ æœ€ä¼˜å€¼æ›´æ–°é€šçŸ¥
                        self._notify_new_best_score(island_id, score, sample)
            # print("ä¸€å…±æœ‰å¤šå°‘ä¸ªsampleï¼Ÿ",i)
           
            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºè¯¦ç»†çš„æ ·æœ¬æ•°æ®
            # print("score_for_sample: ")
            # print(score_for_sample)
            # print("===========error_for_samlple:============================\n ")
            # print(error_for_samlple)
            # print("=========================residual_data: ================\n")
            # print(residual_data)
            for each_score in score_for_sample:
                if each_score == None:
                    quality_for_sample.append('None')
                elif each_score > best_score:
                    quality_for_sample.append('Good')
                else:
                    quality_for_sample.append('Bad')
            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºè´¨é‡æ£€æŸ¥è¯¦æƒ…
            # print("quality_for_sample:")
            # print('================================æ£€æŸ¥ä¸€ä¸‹if_bestçš„å€¼====================\n')
            # print(if_best)
            # è°ƒç”¨åˆ†æå‡½æ•°è¿›è¡Œåˆ†æ
            try:
                #å…ˆç›´æ¥è¿›å…¥ç¬¬ä¸‰æ¬¡
                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºåˆ†æè¿‡ç¨‹è¯¦æƒ…
                # print("\n===== æ–¹ç¨‹å’Œåˆ†æ•°åˆ†æå¼€å§‹ =====")
                # ä½¿ç”¨æ¸…æ´—åçš„æ ·æœ¬è¿›è¡Œåˆ†æï¼Œæé«˜ä¸€è‡´æ€§
                cleaned_samples = [ _sanitize_equation_body(s) for s in samples ]
                analysis_result = self.analyze_equations_with_scores(cleaned_samples, quality_for_sample, error_for_samlple, prompt)
                # print("æ€»çš„åˆ†æç»“æœï¼š---------")
                # print(analysis_result)
                # print("===== æ–¹ç¨‹å’Œåˆ†æ•°åˆ†æç»“æŸ =====\n")

                # æ·»åŠ ç¬¬ä¸‰æ¬¡å¯¹è¯ï¼šæ®‹å·®åˆ†æ
                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ®‹å·®åˆ†æè¯¦æƒ…
                # print("\n===== æ®‹å·®åˆ†æå¼€å§‹ =====")
                # print(residual_data)
                # print(if_best)
                if residual_data is not None and if_best:
                    # åªå¯¹æœ‰æ•ˆæ ·æœ¬è¿›è¡Œæ®‹å·®åˆ†æ
                    if_best = False
                    residual_result = self.analyze_equations_with_residual(best_sample,residual_data)
                    # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ®‹å·®åˆ†æç»“æœ
                    # print(f"æ ·æœ¬æ®‹å·®åˆ†æç»“æœ: {residual_result}")
                    # åˆ›å»ºç›®å½•å­˜æ”¾æ®‹å·®åˆ†æç»“æœ
                    residual_analyze_dir = os.path.join(self.config.results_root or ".", "residual_analyze")
                    if not os.path.exists(residual_analyze_dir):
                        os.makedirs(residual_analyze_dir)
                    
                    json_residual_file = os.path.join(residual_analyze_dir, "residual_analyze.json")
                    
                    # åŠ è½½ç°æœ‰çš„æ®‹å·®åˆ†ææ•°æ®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
                    residual_data_list = []
                    if os.path.exists(json_residual_file):
                        try:
                            with open(json_residual_file, "r", encoding="utf-8") as f:
                                existing_data = json.load(f)
                                if isinstance(existing_data, list):
                                    residual_data_list = existing_data
                        except json.JSONDecodeError:
                            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ–‡ä»¶æ ¼å¼é”™è¯¯ä¿¡æ¯
                           print(f"ç°æœ‰çš„æ®‹å·®åˆ†æJSONæ–‡ä»¶æ ¼å¼æœ‰è¯¯ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                        except Exception as e:
                            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                            print(f"è¯»å–ç°æœ‰æ®‹å·®åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    
                    # åˆ›å»ºæ–°çš„æ®‹å·®åˆ†æè®°å½•
                    current_sample_order = self._get_global_sample_nums() - len(samples) + best_id  # è·å–å½“å‰æ ·æœ¬çš„é¡ºåºå·
                    
                    # è®¡ç®—æ®‹å·®ç»Ÿè®¡ä¿¡æ¯
                    res_values = residual_data[:, -1]  # ç¬¬ä¸‰åˆ—æ˜¯æ®‹å·®å€¼

                    # åˆ›å»ºæ®‹å·®åˆ†ææ•°æ®ç»“æ„
                    residual_record = {
                        "sample_order": current_sample_order,
                        "island_id": prompt.island_id,
                        "equation": best_sample,
                        "analysis": residual_result,
                        "best_score": best_score_for_sample,
                    }
                    
                    # æ·»åŠ åˆ°æ®‹å·®åˆ†ææ•°æ®åˆ—è¡¨
                    residual_data_list.append(residual_record)
  
                    # ä¿å­˜æ›´æ–°åçš„æ®‹å·®åˆ†ææ•°æ®
                    try:
                        with open(json_residual_file, "w", encoding="utf-8") as f:
                            json.dump(residual_data_list, f, ensure_ascii=False, indent=2)
                        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ–‡ä»¶æ“ä½œæˆåŠŸä¿¡æ¯
                        # print(f"æˆåŠŸæ›´æ–°æ®‹å·®åˆ†æJSONæ–‡ä»¶: {json_residual_file}")
                    except Exception as e:
                        # ä¿ç•™é”™è¯¯ä¿¡æ¯ï¼Œä½†ç®€åŒ–è¾“å‡º
                        # print(f"ä¿å­˜æ®‹å·®åˆ†æJSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                        pass

                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºåˆ†æç»“æŸæ ‡è®°
                # print("===== æ®‹å·®åˆ†æç»“æŸ =====\n")

                # åˆ›å»ºç›®å½•å­˜æ”¾åˆ†æç»“æœ
                # import os
                experience_dir = os.path.join(self.config.results_root or ".", "equation_experiences")
                
                json_experience_file = os.path.join(experience_dir, "experiences.json")
                

                # åŠ è½½ç°æœ‰çš„ç»éªŒï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
                experiences_data = {"None": [], "Good": [], "Bad": []}
                if os.path.exists(json_experience_file):
                    try:
                        with open(json_experience_file, "r", encoding="utf-8") as f:
                            existing_data = json.load(f)
                            # ç¡®ä¿é”®å­˜åœ¨
                            for key in ["None", "Good", "Bad"]:
                                if key in existing_data:
                                    experiences_data[key] = existing_data[key]
                    except json.JSONDecodeError:
                        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ–‡ä»¶æ ¼å¼é”™è¯¯ä¿¡æ¯
                        print(f"ç°æœ‰çš„ JSON æ–‡ä»¶æ ¼å¼æœ‰è¯¯ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                    except Exception as e:
                        # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                        print(f"è¯»å–ç°æœ‰ç»éªŒæ–‡ä»¶æ—¶å‡ºé”™: {e}")

                # æ·»åŠ æ–°ç»éªŒ
                for i, (sample_text, quality, analysis, error_msg) in enumerate(zip(cleaned_samples, quality_for_sample, analysis_result, error_for_samlple)):
                    # è·å–å½“å‰æ ·æœ¬çš„ä¿¡æ¯
                    current_sample_order = self._get_global_sample_nums() - len(samples) + i + 1  # è®¡ç®—å½“å‰æ ·æœ¬çš„é¡ºåºå·
                    
                    # ç¡®å®šåˆ†ç±»
                    if quality == 'Good':
                        category = "Good"
                    elif quality == 'Bad':
                        category = "Bad"
                    else:  # 'None'
                        category = "None"
                    
                    # åˆ›å»ºç»éªŒæ•°æ®ç»“æ„
                    experience = {
                        "island_id": prompt.island_id,
                        "analysis": analysis,
                        "sample_order": current_sample_order,  # æ·»åŠ æ ·æœ¬é¡ºåºå·
                        "sample_time": sample_time,
                        # ä»…ä¿å­˜æ¸…æ´—åçš„å‡½æ•°ä½“ï¼Œé¿å…æµ‹è¯•ä»£ç /å±é™©è¯­å¥
                        "equation": sample_text,
                        "score": score_for_sample[i],
                    }
                    # ä¿å­˜è®­ç»ƒæ‹Ÿåˆå‚æ•°
                    try:
                        params_i = opt_params_for_sample[i]
                        if params_i is not None:
                            import numpy as _np
                            experience["fitted_params"] = _np.asarray(params_i).tolist()
                    except Exception:
                        pass
                    
                    # å¯¹äº None ç±»å‹ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                    if category == "None" and error_msg:
                        experience["error"] = error_msg
                    
                    # æ·»åŠ åˆ°ç›¸åº”ç±»åˆ«
                    experiences_data[category].append(experience)

                # ä¿å­˜æ›´æ–°åçš„ç»éªŒæ•°æ®
                try:
                    with open(json_experience_file, "w", encoding="utf-8") as f:
                        json.dump(experiences_data, f, ensure_ascii=False, indent=2)
                    # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ–‡ä»¶æ“ä½œæˆåŠŸä¿¡æ¯
                    # print(f"æˆåŠŸæ›´æ–°ç»éªŒ JSON æ–‡ä»¶: {json_experience_file}")
                except Exception as e:
                    # ç®€åŒ–é”™è¯¯è¾“å‡º
                    # print(f"ä¿å­˜ JSON ç»éªŒæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    pass
            except Exception as e:
                # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                # print(f"æ‰§è¡Œåˆ†ææ—¶å‡ºé”™: {str(e)}")
                pass

    def _get_global_sample_nums(self) -> int:
        return self.__class__._global_samples_nums

    def set_global_sample_nums(self, num):
        self.__class__._global_samples_nums = num

    def _global_sample_nums_plus_one(self):
        self.__class__._global_samples_nums += 1

    def _notify_new_best_score(self, island_id, score, sample):
        """é€šçŸ¥æ–°çš„æœ€ä¼˜åˆ†æ•°"""
        try:
            # æå–æ–¹ç¨‹çš„ä¸»è¦éƒ¨åˆ†
            lines = sample.split('\n')
            main_equation = ""
            for line in lines:
                if 'dv =' in line or 'return' in line:
                    main_equation += line.strip() + "; "

            if not main_equation:
                main_equation = lines[0][:100] + "..." if len(lines) > 0 and len(lines[0]) > 100 else (lines[0] if lines else "Unknown")

            # æ£€æŸ¥æ˜¯å¦æ˜¯å…¨å±€æœ€ä¼˜
            current_global_best = max(self._database._best_score_per_island.values()) if self._database._best_score_per_island else float('-inf')

            if score > current_global_best:
                print(f"ğŸ† NEW GLOBAL BEST: Island {island_id}, Score = {score:.6f}")
                print(f"   Equation: {main_equation[:200]}{'...' if len(main_equation) > 200 else ''}")
            else:
                print(f"ğŸ“ˆ Island {island_id} improved: Score = {score:.6f}")

        except Exception:
            # å¦‚æœæå–æ–¹ç¨‹å¤±è´¥ï¼Œåªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            print(f"ğŸ“ˆ Island {island_id} improved: Score = {score:.6f}")



    # 02 ç‰ˆæœ¬ good/bad/noneçš„åˆ†æ
    def analyze_equations_with_scores(self, samples, quality_for_sample, error_for_sample, prompt):
        """
        è®©å¤§æ¨¡å‹åˆ†ææ–¹ç¨‹åŠå…¶å¾—åˆ†ï¼Œæä¾›æ€è€ƒè¿‡ç¨‹åˆ†æå’Œæ”¹è¿›å»ºè®®
        
        Args:
            samples: ç”Ÿæˆçš„æ–¹ç¨‹æ ·æœ¬åˆ—è¡¨
            scores: å¯¹åº”çš„åˆ†æ•°åˆ—è¡¨
            prompt: åŸå§‹æç¤ºï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡
            
        Returns:
            analysis_result: æ¨¡å‹å¯¹æ–¹ç¨‹çš„åˆ†æç»“æœ
        """

        analysis_results = []

        i = 0

        for sample_each in samples:

            if quality_for_sample[i] == 'Good':
                new__question = f"""
                The optimized function skeleton you just answered scored higher. Please summarize useful experience.
                STRICTLY follow these rules:
                1. Use the exact phrasing "when seeking for the mathematical function skeleton that represents {dependent_name_in_prompt} in{problem_name_in_prompt}, I can ..."
                2. Summarize ONLY the key success factors
                3. You need to make your answer as concise as possible
                """
# 1. when seeking for the mathematical function skeleton that represents acceleration in{problem_name_in_prompt}, I can
# 2. Identify ONE crucial improvement point
# 3. You need to make your answer as concise as possible
            elif quality_for_sample[i] == 'Bad':
                new__question = f"""
                The optimized function skeleton you just answered scored lower. What lessons can you draw from it?
                STRICTLY follow these rules: 
                1. Use the exact phrasing "when seeking for the mathematical function skeleton that represents {dependent_name_in_prompt} in{problem_name_in_prompt}, I can ..."
                2. Identify ONE crucial improvement point
                3. You need to make your answer as concise as possible
                """
            
            elif quality_for_sample[i] == 'None':
                new__question = f"""
                The optimized function skeleton you just answered failed with error: {error_for_sample[i]}, What lessons can you draw from it?
                STRICTLY follow these rules:
                1. Use the exact phrasing "when seeking for the mathematical function skeleton that represents {dependent_name_in_prompt} in{problem_name_in_prompt}, I need ..."
                2. Address the SPECIFIC error: {error_for_sample[i]}
                3. Identify ONE crucial improvement point
                4. You need to make your answer as concise as possible
                """
            i += 1

            analysis_prompt = f"""
            Here's our previous conversation:

            user: {prompt}

            assistant: {sample_each}

            user: {new__question}
            """
            # è°ƒç”¨è¿œç¨‹APIåˆ†æç»“æœ
            try:
                client = SHARED_LLM_CLIENT
                if client is None:
                    raise ValueError("æœªæ³¨å…¥å…±äº« LLM å®¢æˆ·ç«¯ï¼Œè¯·åœ¨ Wrapper ä¸­æ³¨å…¥åå†è¿è¡Œã€‚")
                resp = client.chat([{"role": "user", "content": analysis_prompt}])
                analysis_result = resp.get('content', '') or ''
                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºåˆ†æç»“æœè¯¦æƒ…
                # print(f"åˆ†æç»“æœï¼š{analysis_result}")
                analysis_results.append(analysis_result or "åˆ†æä¸ºç©º")
            except Exception as e:
                # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                # print(f"åˆ†æè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}")
                analysis_results.append(f"åˆ†æè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}")
        return analysis_results

    def analyze_equations_with_residual(self, sample, residual) -> str:
        """
    è®©å¤§æ¨¡å‹æ ¹æ®è¾“å…¥çš„æ®‹å·®åˆ†ææ–¹ç¨‹ï¼Œæä¾›å¯¹æ•°æ®çš„æ€è€ƒè¿‡ç¨‹åˆ†æå’Œæ”¹è¿›å»ºè®®
    
    Args:
        sample: ç”Ÿæˆçš„æ–¹ç¨‹æ ·æœ¬
        residual: è¾“å…¥çš„æ•°æ®æ®‹å·®
        prompt: åŸå§‹æç¤ºï¼Œç”¨äºæä¾›ä¸Šä¸‹æ–‡
            
    Returns:
        analysis_result: æ¨¡å‹å¯¹æ–¹ç¨‹çš„åˆ†æç»“æœ
        """
        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºå‡½æ•°è¿›å…¥æ ‡è®°
       # print("========================è¿›å…¥äº†æ®‹å·®åˆ†æå‡½æ•°========================")
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ®‹å·®æ•°æ®
        # è®¡ç®—æ®‹å·®çš„ç»Ÿè®¡ä¿¡æ¯
        res_values = residual[:, -1]  # ç¬¬ä¸‰åˆ—æ˜¯æ®‹å·®å€¼
        mean_res = np.mean(res_values)
        max_res = np.max(np.abs(res_values))
        std_res = np.std(res_values)
        
        try:
            import json
            import os
            import random
            residual_file = os.path.join(self.config.results_root or ".", "residual_analyze", "residual_analyze.json")
            if os.path.exists(residual_file):
                with open(residual_file ,"r", encoding="utf-8") as f:
                    experiences = json.load(f)
                
                #æå–æœ€åä¸€æ¡ä¿¡æ¯
                if experiences:
                    last_experience = experiences[-1]
                    last_analysis = last_experience.get("analysis", "")
        except Exception as e:
            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
              # print(f"åŠ è½½æ®‹å·®æ•°æ®æ—¶å‡ºé”™: {str(e)}")
              # print("Error details:")
            traceback.print_exc()


        # æ„å»ºåˆ†ææç¤º
        res_analyze = f"""
You are a data analysis expert. I will provide a dataset structure for a damped nonlinear oscillator system as follows:
previous conclusions:{last_analysis}
dataset:{residual}
The equation corresponding to the residuals:{sample}

The first two columns are independent variables:
x(position), 
v(velocity).

The third column is the dependent variable a(acceleration).
The forth column contains residuals (calculated as observed value - predicted value from the equation).
Each row represents a set of independent variables (x, v) and their corresponding dependent variable a value, and the residual value.

Task Requirements:

1.Please help me analyze and summarize the influence of the changes in the values of different independent variables on the dependent variable, 
as well as the possible intrinsic relationships among different independent variables.

Your response only needs to answer your analysis results in the form below, and you don't need to show your analysis process.

"""+"""
2.##Output Format##:
STRICTLY deliver results in the following structured format:

Deliver results in the following structured format:

  "output_format": {
    "analysis": {
      "independent_to_dependent_relationships": {
        "x ": [
          "Hint: Here you need to analyze the functional relationship between x and a in different intervals"
        ],
        "v ": [
          "Hint: Here you need to analyze the functional relationship between v and a in different intervals"
        ]
      },
      "inter_relationships_between_independents": {
        "x vs v": [
          "Hint: Here you need to analyze the possible functional relationship between x and v in different intervals. If not, you can leave it blank."
        ]
      }
    }
  }

        """
        
          # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ®‹å·®æç¤ºè¯è¯¦æƒ…
        # print("========è¿™æ˜¯è¾“å…¥çš„æ®‹å·®æç¤ºè¯==========\n")
        # print(res_analyze)
        # è°ƒç”¨è¿œç¨‹APIåˆ†æç»“æœ
        try:
            client = SHARED_LLM_CLIENT
            if client is None:
                raise ValueError("æœªæ³¨å…¥å…±äº« LLM å®¢æˆ·ç«¯ï¼Œè¯·åœ¨ Wrapper ä¸­æ³¨å…¥åå†è¿è¡Œã€‚")
            resp = client.chat([{"role": "user", "content": res_analyze}])
            analysis_result = resp.get('content', '') or ''
            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ®‹å·®åˆ†æç»“æœè¯¦æƒ…
            # print(f"æ®‹å·®åˆ†æç»“æœï¼š{analysis_result}")
            return analysis_result
        except Exception as e:
            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
            # print(f"æ®‹å·®åˆ†æè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}")
            return f"åˆ†æè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}"


def _extract_body(sample: str, config: config_lib.Config) -> str:
    """
    ä»…æå– equation*(...) çš„å‡½æ•°ä½“ï¼Œä¸¢å¼ƒæµ‹è¯•ä»£ç /é¡¶å±‚è¯­å¥ã€‚
    ä¼˜å…ˆä½¿ç”¨ AST ç²¾ç¡®å®šä½ï¼Œå¤±è´¥åˆ™å›é€€ä¸ºåŸºäºç¼©è¿›çš„ä¿å®ˆæˆªæ–­ã€‚
    """
    import ast, re

    # 1) AST è·¯å¾„ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªä»¥ equation å¼€å¤´çš„å‡½æ•°ï¼Œæˆªå–å…¶å‡½æ•°ä½“ï¼ˆä¿æŒåŸç¼©è¿›ï¼‰
    try:
        tree = ast.parse(sample)
        lines = sample.splitlines()
        target = None
        for node in tree.body:
            if isinstance(node, ast.FunctionDef) and node.name.startswith('equation'):
                target = node
                break
        if target and target.body:
            body_start = target.body[0].lineno - 1
            body_end = target.end_lineno
            body = "\n".join(lines[body_start:body_end]) + "\n"
            return body
    except SyntaxError:
        pass

    # 2) å›é€€ï¼šä» def equationâ€¦: çš„ä¸‹ä¸€è¡Œå¼€å§‹ï¼Œæ”¶é›†è¿ç»­ç¼©è¿›è¡Œï¼Œé‡åˆ°é¡¶å±‚éç¼©è¿›è¡Œåœæ­¢
    lines = sample.splitlines()
    func_def = None
    for i, line in enumerate(lines):
        if line.lstrip().startswith('def ') and re.search(r'\bequation\b', line):
            func_def = i
            break
    if func_def is not None:
        body_lines = []
        indent = '    '
        for line in lines[func_def + 1:]:
            if not line.strip():
                body_lines.append(line)
                continue
            if not line.startswith(indent):
                break
            body_lines.append(line)
        return "\n".join(body_lines) + "\n"

    # 3) æœ€åå›é€€ï¼šè¿”å›åŸæ ·ï¼ˆå°½é‡é¿å…ï¼Œä½†ä¿è¯æµç¨‹ä¸å´©ï¼‰
    return sample



class LocalLLM(LLM):
    def __init__(self, samples_per_prompt: int, batch_inference: bool = True, trim=True) -> None:
        """
        Args:
            batch_inference: Use batch inference when sample equation program skeletons. The batch size equals to the samples_per_prompt.
        """
        super().__init__(samples_per_prompt)

        instruction_prompt = (
            "You are a helpful assistant tasked with discovering mathematical function structures for scientific systems. "
            "Complete ONLY the body lines of the 'equation' function below, considering the physical meaning and relationships of inputs.\n"
            "Rules:\n"
            "- Do not output 'def ...' header, imports, tests, prints, or random sampling.\n"
            "- Use variable names 'col0', 'col1', ... that match the function signature.\n"
            "- Return a numpy array with the same length as inputs.\n\n"
        )
        self._batch_inference = batch_inference
        self._instruction_prompt = instruction_prompt
        self._trim = trim

        ####################################
        # æ·»åŠ ä¼šè¯IDå­˜å‚¨
        # self._conversation_ids = {}  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„å¯¹è¯ID
        
        


    def draw_samples(self, prompt: str, config: config_lib.Config) -> Collection[str]:
        """Returns multiple equation program skeleton hypotheses for the given `prompt`."""
        # è®°å½•ç»Ÿä¸€ç»“æœç›®å½•ä¾›æœ¬åœ°è·¯å¾„å¼•ç”¨
        try:
            self._base_dir = config.results_root or "."
        except Exception:
            self._base_dir = "."
        # ç»‘å®šå…±äº« client
        self._client = SHARED_LLM_CLIENT

        if config.use_api:
            return self._draw_samples_api(prompt, config)
        else:
            return self._draw_samples_local(prompt, config)


    def _draw_samples_local(self, prompt: str, config: config_lib.Config) -> Collection[str]:    
        # instruction
        prompt = '\n'.join([self._instruction_prompt, prompt])
        while True:
            try:
                all_samples = []
                # response from llm server
                if self._batch_inference:

###############################################
                    # ç°åœ¨_do_requestè¿”å›ä¸¤ç»„å“åº”ï¼Œåªå–ç¬¬ä¸€ç»„(æ–¹ç¨‹å®ç°)
                    # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºåˆ†æ”¯è¿è¡Œä¿¡æ¯
                  # print("è¿è¡Œäº†_draw_samples_localçš„_batch_inferenceåˆ†æ”¯")

                    first_responses = self._do_request(prompt)
                    # 01 ç‰ˆæœ¬
                    # first_responses, second_responses = self._do_request(prompt)
                    # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºè¿è¡ŒæˆåŠŸä¿¡æ¯
                    # print("æˆåŠŸè¿è¡Œfirst_responses = self._do_request(prompt)")

                    # all_samples = first_responses


                    # åŸä»£ç 
                    # response = self._do_request(prompt)
                    # for res in response:

#####################################################
                    # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºå“åº”è¯¦æƒ…
                    # print(first_responses)                   
                    for res in first_responses:
                        all_samples.append(res)

                    # print("-------jjjjjjjjjjjjjjjj")
                else:
                    for _ in range(self._samples_per_prompt):
                        # response = self._do_request(prompt)
                        # all_samples.append(response)
                        # print("________")
                        first_responses, second_responses = self._do_request(prompt)
                        all_samples.append(first_responses)

                # print(all_samples)


                # trim equation program skeleton body from samples
                if self._trim:
                    all_samples = [_extract_body(sample, config) for sample in all_samples]
                
                # print("å¤„ç†è¿‡çš„all_samples")
                # print(all_samples)
                

                # 01ç‰ˆæœ¬
                # return all_samples, second_responses
            
                return all_samples
            except Exception:
                continue


    def _draw_samples_api(self, prompt: str, config: config_lib.Config) -> Collection[str]:
        all_samples = []
        full_prompt = '\n'.join([self._instruction_prompt, prompt])
        client = getattr(self, '_client', None)
        if client is None:
            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                  # print("æœªæ³¨å…¥å…±äº« LLM å®¢æˆ·ç«¯ï¼Œæ— æ³•è¿›è¡Œé‡‡æ ·ã€‚è¯·åœ¨ Wrapper ä¸­é€šè¿‡ set_shared_llm_client æ³¨å…¥ã€‚")
            return [""] * self._samples_per_prompt

        for _ in range(self._samples_per_prompt):
            while True:
                try:
                    resp = client.chat([{ "role": "user", "content": full_prompt }])
                    content = resp.get('content', '') or ''
                    if self._trim:
                        content = _extract_body(content, config)
                    all_samples.append(content)
                    break
                except Exception as e:
                    # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                               # print(f"APIè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}")
                    import time as _t
                    _t.sleep(1)
                    continue
        return all_samples
    
    
    def _do_request(self, content: str) -> str:
        content = content.strip('\n').strip()
        # repeat the prompt for batch inference
        repeat_prompt: int = self._samples_per_prompt if self._batch_inference else 1

        # print('ttttttttttttttttttttttttttttttttttttttttt')
        # print(content)
        # print('ttttttttttttttttttttttttttttttttttttttttt')
        # æ·»åŠ  idea_lib

        # å°è¯•åŠ è½½ç»éªŒæ•°æ®
        try:
        # if True:


            # è·å–å½“å‰çš„ sample_order
            # current_sample_order = self._get_global_sample_nums()
            # print(f"å½“å‰ sample_order: {current_sample_order}")
            
            # è®¡ç®—ç»éªŒæ–‡ä»¶ä¸­æ‰€æœ‰ç±»åˆ«ç»éªŒçš„æ€»æ•°
            current_sample_order = 0

            experience_file = os.path.join(getattr(self, "_base_dir", "."), "equation_experiences", "experiences.json")

            if os.path.exists(experience_file):
                with open(experience_file, "r", encoding="utf-8") as f:
                    experiences = json.load(f)
                
                # ç»Ÿè®¡æ‰€æœ‰ç±»åˆ«çš„ç»éªŒæ€»æ•°
                for category in ["None", "Good", "Bad"]:
                    if category in experiences:
                        current_sample_order += len(experiences[category])
                
                # print(f"ç»éªŒåº“ä¸­å…±æœ‰ {current_sample_order} æ¡ç»éªŒ")


                
                # å‡†å¤‡å­˜å‚¨ç­›é€‰åçš„å„ç±»ç»éªŒ
                filtered_experiences = {"None": [], "Good": [], "Bad": []}
                
                # æ ¹æ®å½“å‰ sample_order é€‰æ‹©åˆé€‚çš„ç»éªŒ
                # for category in ["None", "Good", "Bad"]:
                for category in ["None"]:
                    if category in experiences and experiences[category]:
                        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„ç»éªŒ
                        if current_sample_order <= 50:
                            # sample_order < 50 æ—¶ï¼Œä¸é™åˆ¶ç»éªŒçš„ sample_order
                            filtered_category = experiences[category]
                        else:
                            # sample_order > 50 æ—¶ï¼Œåªé€‰æ‹© sample_order åœ¨å½“å‰å€¼çš„ 0.5~1 å€èŒƒå›´å†…çš„ç»éªŒ
                            min_order = current_sample_order * 0.7
                            max_order = current_sample_order
                            filtered_category = [
                                exp for exp in experiences[category] 
                                if "sample_order" in exp and min_order <= exp["sample_order"] <= max_order
                            ]
                        
                        # éšæœºé€‰æ‹©æœ€å¤š3ä¸ªç»éªŒ
                        if filtered_category:
                            selected = random.sample(filtered_category, min(3, len(filtered_category)))
                            filtered_experiences[category] = selected
                
                # åˆå¹¶æ‰€æœ‰ç±»åˆ«çš„ç»éªŒ
                all_selected_experiences = []
                for category, exps in filtered_experiences.items():
                    for exp in exps:
                        experience_entry = {
                            "type": category,
                            "analysis": exp.get("analysis", ""),
                            "sample_order": exp.get("sample_order", "unknown"),
                        }
                        
                        # å¯¹äº None ç±»åˆ«ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        if category == "None" and "error" in exp:
                            error_msg = exp["error"]
                            # ç§»é™¤ç‰¹å®šé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            if error_msg == "Execution Error: too many values to unpack (expected 5)":
                                error_msg = ""
                            
                            if error_msg:
                                experience_entry["error"] = error_msg
                        
                        all_selected_experiences.append(experience_entry)
                
                # å¦‚æœæœ‰ç»éªŒå¯ç”¨ï¼Œæ„å»ºç»éªŒæç¤º
                if all_selected_experiences:
                    experience_prompt = "\n\n### The following are ideas summarized based on past experiences in solving such problems. ###\n\n"
                    
                    # ä¸ºæ¯ä¸ªç»éªŒåˆ†é…ç¼–å·
                    for i, exp in enumerate(all_selected_experiences, 1):
                        experience_prompt += f"idea{i}ï¼š\n"
                        # experience_prompt += f"(sample_order: {exp['sample_order']})\n"
                        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ ·æœ¬é¡ºåºè¯¦æƒ…
                                 # print("=================================sample_order: ==================================\n", exp['sample_order'])
                        
                        # é™åˆ¶ç»éªŒåˆ†ææ–‡æœ¬æœ€å¤š100ä¸ªå­—ç¬¦
                        analysis_text = exp["analysis"] if exp.get("analysis") else ""
                        if len(analysis_text) > 500:
                            analysis_text = analysis_text[:500] + "..."
                        experience_prompt += analysis_text
                        
                        experience_prompt += "\n---\n\n"
                    
                    # å°†ç»éªŒæ·»åŠ åˆ°åŸå§‹å†…å®¹ä¸­
                    content_with_lib = experience_prompt + "\n\n" + content
                    # print(f"å·²æ·»åŠ  {len(all_selected_experiences)} æ¡å†å²ç»éªŒåˆ°æç¤ºä¸­")

                    content = content_with_lib

            #æœ‰pçš„å‡ ç‡è¿›å…¥ä»¥ä¸‹ä»£ç ï¼š
            p = 1.0  # è®¾ç½®æ‰§è¡Œæ¦‚ç‡ä¸º50%ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªå€¼
            
            if random.random() < p and os.path.exists(experience_file):
                # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ®‹å·®åˆ†æä½¿ç”¨æ ‡è®°
                  # print("use residual_analyze: True")

                residual_file = os.path.join(getattr(self, "_base_dir", "."), "residual_analyze", "residual_analyze.json")
                if os.path.exists(residual_file):
                    with open(residual_file ,"r", encoding="utf-8") as f:
                        experiences = json.load(f)
                    
                    #æå–æœ€åä¸€æ¡ä¿¡æ¯
                    if experiences:
                        last_experience = experiences[-1]
                        last_analysis = last_experience.get("analysis", "")
                        last_sample_order = last_experience.get("sample_order", "unknown")
                        last_equation = last_experience.get("equation", "")
                        if last_equation is not None:
                        
                            # æ„å»ºæç¤º
                            experience_prompt = f"\n\n### The following is the analysis result of the existing data on{problem_name_in_prompt}, which will assist you in answering the question. ###\n\n"
                            # experience_prompt += f"ç»éªŒ{last_sample_order}ï¼š\n"
                            # experience_prompt += f"(sample_order: {last_sample_order})\n"
                            if len(last_analysis) > 2000:
                                last_analysis = last_analysis[:2000] + "..."
                            experience_prompt += last_analysis
                            # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæ ·æœ¬é¡ºåºè¯¦æƒ…
                                     # print("=================================sample_order: ==================================\n", last_sample_order)
                            # å°†ç»éªŒæ·»åŠ åˆ°åŸå§‹å†…å®¹ä¸­
                            content_with_residual = experience_prompt + "\n\n" + content

                            content = content_with_residual
                        
                            # print(f"æ®‹å·®åˆ†æåº“ä¸­å…±æœ‰ {len(experiences)} æ¡ç»éªŒ")
                        
                        else:
                            # æ„å»ºæç¤º
                            experience_prompt = f"\n\n### The following is the analysis result of the existing data on{problem_name_in_prompt}, which will assist you in answering the question. ###\n\n"
                            # experience_prompt += f"ç»éªŒ{last_sample_order}ï¼š\n"
                            # experience_prompt += f"(sample_order: {last_sample_order})\n"
                            if isinstance(last_analysis, list):
                                last_analysis = last_analysis[0] if last_analysis else ""
                            # print(f"===========================last_analysis:=====================================\n {last_analysis}")
                            if len(last_analysis) > 2000:
                                last_analysis = last_analysis[:2000] + "..."
                            experience_prompt += last_analysis

                            # å°†ç»éªŒæ·»åŠ åˆ°åŸå§‹å†…å®¹ä¸­
                            content_with_residual = experience_prompt + "\n\n" + content

                            content = content_with_residual
                        
                            # print(f"æ®‹å·®åˆ†æåº“ä¸­å…±æœ‰ {len(experiences)} æ¡ç»éªŒ")


        except Exception as e:
            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                # print(f"åŠ è½½ç»éªŒæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                # print("Error details:")
            traceback.print_exc()  # è¾“å‡ºè¯¦ç»†çš„é”™è¯¯å †æ ˆä¿¡æ¯
        
        # é‡å¤æç¤ºä»¥è¿›è¡Œæ‰¹é‡æ¨ç†
        repeat_prompt: int = self._samples_per_prompt if self._batch_inference else 1


        head = f"""
        Find the mathematical function skeleton that represents acceleration in{problem_name_in_prompt} with driving force, given data on {independent_name_in_prompt}. 
        """
        content = head +'\n'+ content
        # é™é»˜åŒ–ï¼šä¸æ˜¾ç¤ºæœ€ç»ˆè¾“å…¥å†…å®¹
            # print("========================æœ€ç»ˆè¾“å…¥ç»™å¤§æ¨¡å‹çš„content========================\n")
            # print(content)

        responses = []
        client = SHARED_LLM_CLIENT
        if client is None:
            # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                   # print("æœªæ³¨å…¥å…±äº« LLM å®¢æˆ·ç«¯ï¼Œæ— æ³•è¿›è¡Œè¯·æ±‚ã€‚è¯·åœ¨ Wrapper ä¸­é€šè¿‡ set_shared_llm_client æ³¨å…¥ã€‚")
            return [""] * repeat_prompt if self._batch_inference else ""

        for _ in range(repeat_prompt):
            try:
                resp = client.chat([{ "role": "user", "content": content }])
                responses.append(resp.get('content', '') or '')
            except Exception as e:
                # é™é»˜åŒ–ï¼šç®€åŒ–é”™è¯¯è¾“å‡º
                               # print(f"APIè¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}")
                responses.append("")

        return responses if self._batch_inference else responses[0]
